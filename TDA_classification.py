"""
PART IV - Classification on TDA landscapes
"""
## Part IVa - hierarchical clustering

import matplotlib.pyplot as plt
import numpy as np
import os

# Read in list of structures
path_to_GCD = 'GCD.gcd' # MODIFY THIS
gcd_list = open(path_to_GCD, 'r').read()
refcodes = gcd_list.split("\n")

# Create list of indices for pandas dataframe (not using the refcodes list
# in case something wrong happens)
indices = []
shapes = {}

# Check the shapes of the structures landscapes
for refcode in refcodes:
    try:
        if os.path.exists(refcode+".npy"):
            with open(refcode+".npy", 'rb') as f:
                L = np.load(f)
                shapes[refcode] = L.shape
            indices.append(refcode)
    except:
        print("issue with", refcode)

# Create a master array where each row is a landscape
master = np.empty((1, 50))
indices_100 = []
for refcode in shapes.keys():
    if shapes[refcode][0] == 100:
        with open(refcode+".npy", 'rb') as f:
            L = np.load(f)
            master = np.vstack([master, L])
            indices_100.append(refcode)

# Remove first row of approx zeros generated by np.empty
master = np.delete(master, 0, 0)

# create list of columns for pandas dataframe
columns = []
for i in range(50):
    columns.append("dimension 1 " + str(i))
for i in range(50):
    columns.append("dimension 2 " + str(i))

# Convert to pandas dataframe and save
panda_master = pd.DataFrame(data=master, index=indices_100, columns=columns)
panda_master.to_csv('cages_final.csv')

# Computing dendrogram
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import squareform

# Compute linkage
Z = linkage(panda_master, 'ward')

# This function is used later on for plotting dendrogram:
def fancy_dendrogram(*args, **kwargs):
    max_d = kwargs.pop('max_d', None)
    if max_d and 'colour_threshold' not in kwargs:
        kwargs['colour_threshold'] = max_d
    annotate_above = kwargs.pop('annotate_above', 0)

    ddata = dendrogram(*args, **kwargs)

    if not kwargs.get('no_plot', False):
        plt.title('Hierarchical Clustering Dendrogram (truncated)')
        plt.xlabel('Cluster size')
        plt.ylabel('Distance')
        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['colour_list']):
            x = 0.5 * sum(i[1:3])
            y = d[1]
            if y > annotate_above:
                plt.plot(x, y, 'o', c=c)
                plt.annotate("%.3g" % y, (x, y), xytext=(0, -5),
                             textcoords='offset points',
                             va='top', ha='center')
        if max_d:
            plt.axhline(y=max_d, c='k')
    return ddata

# look at the dendrogram and decide on a max_d
max_d = 30

# Plot dendrogram with max_d
fancy_dendrogram(
    Z,
    truncate_mode='lastp',
    p=48,
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,
    annotate_above=30,
    max_d=max_d,  # plot a horizontal cut-off line
)
plt.show()

# See number of clusters k
k=11
clusters = fcluster(Z, k, criterion='maxclust')

# Then assemble data into a dataframe with refcodes and the corresponding predicted class
labeled = np.array([indices_100, clusters])
labeled = labeled.T
labeled_pd = pd.DataFrame(data = labeled, columns =['refcode', 'class'])

# To see the data in a dictionary format
classes = {}
for i in range(1,19):
    classes[i] = labeled_pd.loc[labeled_pd['class'] == str(i)]

# Save results as GCD for easy visualisation
for i in range(1,19):
    for j in range(classes[i].shape[0]):
        w = csv.writer(open("class_11"+str(i)+".gcd", "a"))
        w.writerow([classes[i].iloc[j][0]])

## Part IVb - Random forest

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

# importing dataframe and selecting columns
dataset=pd.read_csv('OC_NOC.csv')
dataset.head()

X = dataset.iloc[:-1,1:-1]
Y = dataset.iloc[:-1,-1:]
Y = Y.values.ravel()

# setting an out-split to validate performance after cv
X_cv, X_out,Y_cv, Y_out = train_test_split(X,Y,test_size=0.15, random_state=42)

#Base model
clf=RandomForestClassifier(n_estimators=100, random_state=75)
clf.fit(X_cv, Y_cv)
Y_pred=clf.predict(X_out)
all_pred=clf.predict(X)

print("Accuracy:",metrics.accuracy_score(Y_out, Y_pred))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_out, Y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(Y_out, Y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_out, Y_pred)))
print('R Squared:', metrics.r2_score(Y_out, Y_pred))
